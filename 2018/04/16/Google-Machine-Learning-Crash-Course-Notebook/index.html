<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="“可与言而不与之言，失人；不可与言而与之言，失言。” ——《论语·卫灵公》"><title>谷歌机器学习速成课程 (Google Machine Learning Crash Course) 笔记 | DeeCheng's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">谷歌机器学习速成课程 (Google Machine Learning Crash Course) 笔记</h1><a id="logo" href="/.">DeeCheng's Blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">谷歌机器学习速成课程 (Google Machine Learning Crash Course) 笔记</h1><div class="post-meta">Apr 16, 2018</div><a class="disqus-comment-count" data-disqus-identifier="2018/04/16/Google-Machine-Learning-Crash-Course-Notebook/" href="/2018/04/16/Google-Machine-Learning-Crash-Course-Notebook/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#课程概述"><span class="toc-number">1.</span> <span class="toc-text">课程概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#机器学习基本术语以及深入"><span class="toc-number">2.</span> <span class="toc-text">机器学习基本术语以及深入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#降低损失和泛化"><span class="toc-number">3.</span> <span class="toc-text">降低损失和泛化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensorflow"><span class="toc-number">4.</span> <span class="toc-text">Tensorflow</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#表示法"><span class="toc-number">5.</span> <span class="toc-text">表示法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集划分"><span class="toc-number">5.1.</span> <span class="toc-text">数据集划分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征工程"><span class="toc-number">5.2.</span> <span class="toc-text">特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#映射过程"><span class="toc-number">5.2.1.</span> <span class="toc-text">映射过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#特殊值问题"><span class="toc-number">5.2.2.</span> <span class="toc-text">特殊值问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据清理"><span class="toc-number">5.2.3.</span> <span class="toc-text">数据清理</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#缩放特征值"><span class="toc-number">5.2.3.1.</span> <span class="toc-text">缩放特征值</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#处理极端离群值"><span class="toc-number">5.2.3.2.</span> <span class="toc-text">处理极端离群值</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#分格"><span class="toc-number">5.2.3.3.</span> <span class="toc-text">分格</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#清查"><span class="toc-number">5.2.3.4.</span> <span class="toc-text">清查</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征组合"><span class="toc-number">5.3.</span> <span class="toc-text">特征组合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#泛化"><span class="toc-number">6.</span> <span class="toc-text">泛化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ROC和AUC"><span class="toc-number">7.</span> <span class="toc-text">ROC和AUC</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络"><span class="toc-number">8.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#学习速率"><span class="toc-number">8.1.</span> <span class="toc-text">学习速率</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">9.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><h2 id="课程概述"><a href="#课程概述" class="headerlink" title="课程概述"></a>课程概述</h2><p>在 @stormzhang 的公众号发现了这份机器学习的学习资料， 学习之后颇有收获，遂记录下来成此文章</p>
<blockquote>
<p>Google 发布了机器学习的速成课，这门课程是基于 TensorFlow 的，它包含 40 多项练习、25 节课程以及 15 个小时的紧凑学习内容。据说，这份资料原本是 Google 内部员工的培训资料，后来效果不错，Google 就决定把它公开出来，而且免费，这还不够，Google 还提供了中文资料，最关键的是Google 还为视频配备了中文音频，虽然是机器翻译的，没那么人性化，但也可以说是非常贴心了</p>
</blockquote>
<p>谷歌机器学习速成课程属于此领域综述性的课程，面面俱到而且能够通过实际的例子把机器学习的内容呈现出来，使学习者能够具体地从一个小项目中（课程中使用了加利福尼亚州在1990年的人口住房普查数据）实际动手（课程中提供学习者可以调整模型参数的 Playground 练习以及可以在 <a href="https://colab.research.google.com/" target="_blank" rel="noopener">Colaboratory</a> 平台直接运行的编程练习）理解机器学习知识，而且课程处处显现着谷歌在处理大数据的过程中对机器学习实战的理解（比如对机器学习模型复杂度的拿捏方面），这对初学者来说这些概念虽然十分抽象但是对初学者以后的处理实际数据的过程是有益的。</p>
<p>谷歌这门课程 和 <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">吴恩达机器学习公开课</a> 最大的区别在吴恩达课程中会实际讲解梯度下降算法的公式以及计算过程并且使用Matlab来实现算法，而本课程并不涉及算法的具体内容，而是通过使用Tensorflow所提供的API来简化模型的搭建过程从而向初学者展示机器学习的 <strong>Big Picture</strong></p>
<p>下文中我会引用课程的基本目录结构结合我自己的实践经验来记述。</p>
<h2 id="机器学习基本术语以及深入"><a href="#机器学习基本术语以及深入" class="headerlink" title="机器学习基本术语以及深入"></a>机器学习基本术语以及深入</h2><p>谷歌在这两节中介绍了一些机器学习的基本术语（标签、特征、样本、模型、学习速率以及回归与分类的区别）以及一些相关模型训练过程中使用到的一些基本概念（线性回归、梯度下降以及对训练模型准确度的评价指数）<img src="https://i.loli.net/2018/06/11/5b1dcfd3acb4a.png" alt="不同损失指数的模型对比"></p>
<h2 id="降低损失和泛化"><a href="#降低损失和泛化" class="headerlink" title="降低损失和泛化"></a>降低损失和泛化</h2><p>这章叙述的是在我们实际训练模型过程中使用到的一些算法（主要是梯度下降算法及其变形）以及学习速率的调整的技巧, 标题中说到的两个概念是相反的过程，降低损失要求我们需要越来越精确地调整机器学习算法所生成的模型以迎合所使用的数据集，但是这个调整过程存在着过拟合的风险，所以我们需要泛化项（课程中使用O(Model Complexity)模型复杂度参数来制衡过拟合）来精简我们的生成模型。<br><img src="https://i.loli.net/2018/06/11/5b1dd0e46b504.png" alt="机器学习算法用于训练模型的迭代试错过程"></p>
<h2 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h2><p>这章简要介绍Tensorflow框架的基本信息以及tf.estimator API，课程中使用 tf.estimator 来完成大部分练习，这样可以使我们可以注重模型建立的过程而不是纠结算法的编程实现，同时大大简化训练模型的代码行，本课程注重的是Tensorflow高层API的调用过程。<br><img src="https://i.loli.net/2018/06/11/5b1dd1178fd6e.png" alt="TensorFlow 工具包层次结构"></p>
<h2 id="表示法"><a href="#表示法" class="headerlink" title="表示法"></a>表示法</h2><p>在这章主要介绍了我们从原始数据（混杂各种各种数值以及字符串信息）到编码导入过程，介绍了一些数据到向量转换过程中所用到的一些方法技巧。</p>
<p>对于机器学习模型建立的过程来说，数据以及数据结构的优良性直接决定了模型生成过程以及参数调优的过程，决定了耗损函数所能达到的最低值。机器学习无法根据给出的一批杂乱无章的数据生成模型，或者就算可以生成那么这个模型也是站不住脚的，人们使用新的数据去验证它将无法得到满意的结果，<strong>耗损指数（Loss Value）</strong> 会相当高，所以数据编码清理阶段是机器学习模型建立中十分重要的一环。</p>
<h3 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h3><p>我们可以将一批数据拆分为训练集、验证集（我们引用验证集来降低过拟合的发生几率）以及测试集，以下为附带验证集的模型建立工作流程图</p>
<blockquote>
<p>不断使用测试集和验证集会使其逐渐失去效果。也就是说，您使用相同数据来决定超参数设置或其他模型改进的次数越多，您对于这些结果能够真正泛化到未见过的新数据的信心就越低。请注意，验证集的失效速度通常比测试集缓慢。</p>
</blockquote>
<p><img src="https://i.loli.net/2018/06/11/5b1dd11a56ec5.png" alt="附带验证集的模型建立工作流程"></p>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>传统编程的关注点是代码编码过程，而机器学习项目工作者的关注点是数据表示过程。也就是说，开发者通过添加和改善特征（特征可以来源与单个标签或者多个标签的逻辑运算）来调整模型，一个好的数据特征原型对机器学习模型的生成十分重要。特征工程主要的工作一是映射数值，二是采用One-Hot Coding方式映射字符串值以及枚举值（模型无法通过字符串值学习规律，因此需要进行一些特征工程来将这些值转换为数字形式）<br><img src="https://i.loli.net/2018/06/11/5b1dd11c7db12.png" alt="程序工程将原始数据映射到机器学习特征"></p>
<h4 id="映射过程"><a href="#映射过程" class="headerlink" title="映射过程"></a>映射过程</h4><blockquote>
<p>在字符串映射的过程中最好不要使用具体数字去定义特定的物体（比如某条街道的名称），这样不利于后期对特征作交并集处理，而是判断一个物体$x_1$是否属于一个集合，这也是One-Hot Coding的一种方式</p>
</blockquote>
<p>比如我们需要定义 <code>Lowland Countries</code> 的三个特征</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&apos;Netherlands&apos;, &apos;Belgium&apos;, &apos;Luxembourg&apos;&#125;</span><br></pre></td></tr></table></figure>
<p>那么我们可以将每个分类特征表示为单独的布尔值从而把 <code>Lowland Countries</code> 在模型中表现为三个单独的布尔值特征</p>
<ul>
<li>x1：是荷兰吗？</li>
<li>x2：是比利时吗？</li>
<li>x3：是卢森堡吗？</li>
</ul>
<p>这样当我们需要处理“与法国接壤”这种特殊情况的时候，我们就可以将比利时和卢森堡都设置为 <code>True</code> 来表示。</p>
<h4 id="特殊值问题"><a href="#特殊值问题" class="headerlink" title="特殊值问题"></a>特殊值问题</h4><p>我们不该使用一些使用的离散特征值（某个特征的值仅在整个数据集中出现一次或者很少出现），这些离散特征值更像是一些噪声数据。同样我们也不应该使用-1去表示一个标签的值缺失，这样不利于机器学习到数据的固定模式，我们可以定义一个布尔键位完成这项工作。良好的特征矢量有以下的特点：</p>
<ul>
<li>避免很少使用的离散特征值</li>
<li>最好具有清晰明确的含义</li>
<li>不要将“神奇”的值与实际数据混为一谈（良好的浮点特征不包含超出范围的异常断点）</li>
<li>考虑上游不稳定性（特征的定义不应随时间发生变化）</li>
</ul>
<h4 id="数据清理"><a href="#数据清理" class="headerlink" title="数据清理"></a>数据清理</h4><p>干净的数据集对于机器学习模型的建立至关重要，维持数据集需要注意以下几个方面：</p>
<ul>
<li><p>数据可靠性 </p>
<ul>
<li>机器学习开发者需要专注于来源数据的可靠性问题，如果数据来源不可靠的话后期的模型建立的方法对数据预测的准确性帮助并不大</li>
</ul>
</li>
<li><p>数据的版本控制 </p>
<ul>
<li>不同的数据对机器学习模型的生成影响很大，如果输入的数据不断发生变化的话，很可能无法生成正确的模型。上游进程的突然变化可能会波及模型的建立过程</li>
</ul>
</li>
<li><p>特征必要性问题</p>
<ul>
<li>机器建模的过程中不见得越多特征，建立的模型就越可靠；特征增加的过程同时也是维护负担增加的过程</li>
</ul>
</li>
<li><p>数据的相关性</p>
</li>
</ul>
<p>我们可以对数据集进行调整以更好地被学习到数据集的固定模式</p>
<h5 id="缩放特征值"><a href="#缩放特征值" class="headerlink" title="缩放特征值"></a>缩放特征值</h5><p>缩放是指将浮点特征值从自然范围（例如 100 到 900）转换为标准范围（例如 0 到 1 或 -1 到 +1）</p>
<p>则缩放特征可以带来以下优势：</p>
<ul>
<li>帮助梯度下降法更快速地收敛。</li>
<li>帮助避免 <code>NaN 陷阱</code>。在这种陷阱中，模型中的一个数值变成 <code>NaN</code>（例如，当某个值在训练期间超出浮点精确率限制时），并且模型中的所有其他数值最终也会因数学运算而变成 <code>NaN</code>。</li>
<li>帮助模型为每个特征确定合适的权重。如果没有进行特征缩放，则模型会对范围较大的特征投入过多精力。</li>
</ul>
<h5 id="处理极端离群值"><a href="#处理极端离群值" class="headerlink" title="处理极端离群值"></a>处理极端离群值</h5><p>课程中以加利福尼亚州住房数据集中 <code>roomsPerPerson</code> 的特征来说明，此方法的总体思想是将偏离中 心分布而又及其少量的数据往中心分布取简. 由上图可以知道房价呈现<strong>正态分布</strong>。下面还可以看到两个统计学中的概念：</p>
<p><strong>峰度（Kurtosis）</strong> 和 <strong>偏度（Skewness）</strong></p>
<blockquote>
<p><strong>峰度：峰度（Kurtosis）是描述某变量所有取值分布形态陡缓程度的统计量。</strong></p>
<p>它是和正态分布相比较的</p>
<ul>
<li><p>Kurtosis=0 与正态分布的陡缓程度相同。</p>
</li>
<li><p>Kurtosis&gt;0 比正态分布的高峰更加陡峭——尖顶峰</p>
</li>
<li><p>Kurtosis&lt;0 比正态分布的高峰来得平台——平顶峰计算公式：</p>
</li>
</ul>
<p><strong>$$β = M_4 /σ^4$$</strong></p>
<p><strong>偏度：偏度（Skewness）是描述某变量取值分布对称性的统计量。</strong></p>
<ul>
<li>Skewness=0 分布形态与正态分布偏度相同</li>
<li>Skewness&gt;0 正偏差数值较大，为正偏或右偏。长尾巴拖在右边。</li>
<li>Skewness&lt;0 负偏差数值较大，为负偏或左偏。长尾巴拖在左边。 计算公式：<br><strong>$$S= (X^ - M_0)/δ$$</strong></li>
</ul>
<p>Skewness 越大，分布形态偏移程度越大。</p>
</blockquote>
<p><img src="https://i.loli.net/2018/06/11/5b1dd11daba22.png" alt="数据分布带有一个非常非常长的尾巴"></p>
<p><img src="https://i.loli.net/2018/06/11/5b1dd1eec9000.png" alt="将特征值限制到 4.0"></p>
<h5 id="分格"><a href="#分格" class="headerlink" title="分格"></a>分格</h5><p>在数据集中，如果单为经度或者纬度他们是一个浮点值，故与房屋价格之间并不存在线性关系，我们可以通过对纬度进行分格或者下文提到的<strong>特征组合（通过经纬度组合表示地区）</strong>而使其变为一项使用的预测指标。</p>
<p><img src="https://i.loli.net/2018/06/11/5b1dd1288645e.png" alt="分格值"></p>
<h5 id="清查"><a href="#清查" class="headerlink" title="清查"></a>清查</h5><p>在现实生活中，数据集中的很多样本是不可靠的，一旦检测到存在这些问题，我们通常需要将相应样本从数据集中移除，从而“修正”不良样本。</p>
<p>造成数据样本不可靠的原因有以下一种或多种：</p>
<ul>
<li>遗漏值。 例如，有人忘记为某个房屋的年龄输入值。</li>
<li>重复样本。 例如，服务器错误地将同一条记录上传了两次。</li>
<li>不良标签。 例如，有人错误地将一颗橡树的图片标记为枫树。</li>
<li>不良特征值。 例如，有人输入了多余的位数，或者温度计被遗落在太阳底下。</li>
</ul>
<p>我们可以通过以下的一些常用的统计指标去发现数据集中存在的问题。</p>
<ul>
<li>最大值和最小值</li>
<li>均值和中间值</li>
<li>标准偏差</li>
</ul>
<h3 id="特征组合"><a href="#特征组合" class="headerlink" title="特征组合"></a>特征组合</h3><p>特征组合可以将非线性学习过程例如二次函数分布的数据集学习过程变成线性函数的学习过程，线性函数在面对众多大数据集的时候有更加良好的拉伸特性，我们利用线性特性的交叉特性去逼近非线性的数据集模型。在下图我的测试中，单凭一次项系数 $x_1$ 和 $x_2$ 无法利用神经网络去逼近一个近乎二次分布 $x_1^2+x_2^2=r$ 的数据集，背景色在全部区域上渐渐淡出意味着我们的模型对预测此数据集越来越没有信心，我们需要更好的基础特性或者其他的人造特性。</p>
<blockquote>
<p>You can think of the color strength as suggesting the model’s confidence in its guess. So solid blue means that the model is very confident about its guess and light blue means that the model is less confident.</p>
</blockquote>
<p><img src="https://i.loli.net/2018/06/11/5b1dd129ac18c.png" alt="一次项系数无法逼近二次分布的数据集"></p>
<p>但是特性组合不宜太过复杂，机器学习的是数据集中固定数据出现模式。例如 <code>One Feature Cross: [latitude X longitude X roomsPerPerson]</code></p>
<blockquote>
<p>The data in this exercise is basically linear data plus noise. If we use a model that is too complicated, such as one with too many crosses, we give it the opportunity to fit to the noise in the training data, often at the cost of making the model perform badly on test data.</p>
</blockquote>
<h2 id="泛化"><a href="#泛化" class="headerlink" title="泛化"></a>泛化</h2><p>过度复杂的模型虽然可以将针对当前训练的数据集的耗损函数值降到最低但是过度复杂的预测模型很有可能无法应对新加入的数据，我们可以在 <code>耗损函数</code> 后面添加惩罚项来阻止耗损值下降的过程，这种方法可以防止我们的模型变得十分复杂从而出现对数据集的过拟合，但同时也降低模型对数据集复杂模式的侦测能力，后面的超参 $\lambda$ 是需要我们根据数据集的复杂程度调节。</p>
<blockquote>
<p>While test loss decreases, training loss actually increases. This is expected, because you’ve added another term to the loss function to penalize complexity.</p>
</blockquote>
<p>$$minimize(Loss(Data|Model) + \lambda complexity(Model))$$</p>
<p>泛化增加了神经网络权值矩阵的稀疏度，程度会随着 $L_1$ 和 $L_2$ 泛化器种类而有所不同</p>
<ul>
<li>$L_2$ 惩罚的是权值的平方项 $weight^2$</li>
<li>$L_1$ 惩罚的是权值的绝对值项 $|weight|$</li>
</ul>
<blockquote>
<p>You can think of the derivative of L2 as a force that removes x% of the weight every time. As Zeno knew, even if you remove x percent of a number billions of times, the diminished number will still never quite reach zero</p>
</blockquote>
<p><img src="https://i.loli.net/2018/06/11/5b1de0539c970.png" alt="$L_1$ 和 $L_2$ 泛化器"></p>
<p><img src="https://i.loli.net/2018/06/11/5b1de1ba3eaa3.png" alt="泛化器以及$\lambda$对输出的影响"></p>
<h2 id="ROC和AUC"><a href="#ROC和AUC" class="headerlink" title="ROC和AUC"></a>ROC和AUC</h2><p>ROC曲线：接收者操作特征曲线（receiver operating characteristic curve），是反映敏感性和特异性连续变量的综合指标，roc曲线上每个点反映着对同一信号刺激的感受性。而AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。</p>
<ul>
<li>假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类</li>
</ul>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>前面讲到我们可以根据数据及的分布情况来具体选择模型基底，但是对于大量数据集我们无法一一检视数据集分布情况，这时我们就需要使用神经网络来构建我们的模型，使用后向传输算法训练的神经网络尤其善于侦测复杂数据集的固定模式</p>
<p><img src="https://i.loli.net/2018/06/11/5b1de2934677b.png" alt="不同参数以及激活层对数据集分类的影响"></p>
<h3 id="学习速率"><a href="#学习速率" class="headerlink" title="学习速率"></a>学习速率</h3><p>学习速率是神经网络中十分重要的参数。学习速率增大会影响损耗函数的落点，需要增加多次起始值实验或者调低学习速率，根据最速下降的理论，过大的学习速率可能会在下山的过程中产生剧烈的震荡从而无法，误入歧途的生成模型再也无法达到耗损函数所能达到的最低点（如下图所示）</p>
<p><img src="https://i.loli.net/2018/06/11/5b1df5f23e31a.png" alt="神经网络的模型生成不稳定"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Machine Learning 的万里长征第一步，不过学完这套课程还是很虚，需要去 <a href="https://www.kaggle.com" target="_blank" rel="noopener">Kaggle</a> 多找一些实验数据跑跑才好, 课程最后也给出了一些进阶的学习材料：</p>
<ul>
<li><a href="https://www.udacity.com/course/deep-learning--ud730" target="_blank" rel="noopener">Deep Learning</a>: Advanced machine learning course on neural networks, with extensive coverage of image and text models</li>
<li><a href="https://developers.google.com/machine-learning/rules-of-ml" target="_blank" rel="noopener">Rule of ML</a>: Best practices for machine learning engineering</li>
<li><a href="https://js.tensorflow.org/" target="_blank" rel="noopener">TensorFlow.js</a>: WebGL-accelerated, browser-based JavaScript library for training and deploying ML models</li>
</ul>
<p><strong>Changelog:</strong><br>2018年04月16日 生成初稿<br>2018年6月11日 新增章节并修复问题</p>
</div><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Google/">Google</a></div><div class="post-nav"><a class="next" href="/2018/01/14/Humanity-Education-in-American-University/">阅读经典：美国大学的人文教育(博雅人文)(徐贲) 摘录</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://deecheng.com/2018/04/16/Google-Machine-Learning-Crash-Course-Notebook/';
    this.page.identifier = '2018/04/16/Google-Machine-Learning-Crash-Course-Notebook/';
    this.page.title = '谷歌机器学习速成课程 (Google Machine Learning Crash Course) 笔记';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//imcdee.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//imcdee.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://imcdee.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://deecheng.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/HelloWorld/" style="font-size: 15px;">HelloWorld</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Reading/" style="font-size: 15px;">Reading</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Google/" style="font-size: 15px;">Google</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/16/Google-Machine-Learning-Crash-Course-Notebook/">谷歌机器学习速成课程 (Google Machine Learning Crash Course) 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/14/Humanity-Education-in-American-University/">阅读经典：美国大学的人文教育(博雅人文)(徐贲) 摘录</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/11/Hello-World/">新的开始</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//imcdee.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://jeffjade.com/" title="晚晴幽草轩" target="_blank">晚晴幽草轩</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">DeeCheng's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>